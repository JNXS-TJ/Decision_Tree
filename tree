# Created by Tao Jian in 2019 4 9.
# -*- coding: utf-8 -*-

class Node(object):
    def __init__(self, isLeaf=True, attr=None, vals=[], label=None):
        # isLeaf represent whether the node is a leaf node or not.
        # attr is by which attribute the data set are divided.
        # vals represent the different values of data set in attr.
        self.isLeaf = isLeaf
        self.attr = attr
        self.child = []
        self.vals = vals
        self.label = label
        return

class Decision_Tree(object):
    def __init__(self, X, y, is_discrete):
        # X represent the training set, and it is a NxD array. N represent the number of datas and
        # D represent the number of attributes.
        # y represent corresponding labels.
        self.root = None
        self.X = X
        self.y = y
        self.is_discrete = is_discrete
        return
    
    def is_same_class(self, y):
        # This function judge whether all the smaples of D belong to the same class or not.
        label_compare = y[0]
        for label in y:
            if label != label_compare:
                return False
        return True
    
    def is_same_on_A(self, X, A):
        # This function judge whether all the samples of D have equivalent attributes in A.
        for i in A:
            tmp = X[0][i]
            for j in range(len(X)):
                if tmp != X[j][i]:
                    return False
        return True
    
    def info_entropy(self, y):
        # This function compute information entropy of y.
        # I use log function. It doesn't matter no matter you use log or log2 function.
        uniques, nums = np.unique(y, return_counts=True)
        num_ratio = nums / float(len(y))
        return -np.sum(np.log(num_ratio) * num_ratio)
    
    def biggest_info_gain(self, X, y, A):
        # This function find which attribute make information gain biggest and
        # returns which attribute attain biggest information gain.
        Ent_D = self.info_entropy(y)
        max_info_gain = 0
        which_attr = None
        for attr in A:
            Ent_attr = 0
            unique, nums = np.unique(X[:, attr], return_counts = True)
            # unique represent the different values of X in attr. It is a numpy array.
            # nums represent the numbers of corresponding value of X in attr.
            num_ratio = nums / float(len(y))
            for i in range(len(unique)):
                Ent_attr += self.info_entropy(y[X[:, attr] == unique[i]])*num_ratio[i]
            if max_info_gain < Ent_D - Ent_attr:
                max_info_gain = Ent_D - Ent_attr
                which_attr = attr
        return which_attr
    
    def biggest_info_gain_ratio(self, X, A):
        # This function find which attribute make information gain ratio biggest.
        return
    
    def Gini(self, y):
        # This funtion compute Gini index of y.
        uniques, nums = np.unique(y, return_counts = True)
        num_ratio = nums / len(y)
        return 1 - num_ratio * num_ratio
    
    def Gini_index(self, X, y, A):
        # A represent available attributes.
        # This funtion return which attribute attain the maximum Gini index.
        min_Gini = 9999999999
        which_attr = None
        for attr in A:
            Gini_attr = 0
            uniques, nums = np.unique(X[:, attr], return_counts = True)
            num_ratio = nums / len(y)
            for i in range(len(uniques)):
                val = uniques[i]
                Gini_attr += self.Gini(y[X[:, attr] == val]) * num_ratio[i]
            if Gini_attr < min_Gini:
                min_Gini = Gini_attr
                which_attr = attr
        return which_attr
    
    def build_tree(self):
        A = range(self.X.shape[1])
        self.root = self.generate_tree(self.X, self.y, A, np.arange(len(self.y))+1)
        
    def generate_tree(self, X, y, A, index):
        # A represent the available attributes. A is a list. For example, A = [0, 2, 3], it means that
        # 0-th, 2-th and 3-th attribute are available.
        
        # If all the samples of X belong to the same class, then return a leaf node.
        if self.is_same_class(y):
            #print(index)
            return Node(isLeaf=True, attr=None, vals=[], label=y[0])
        
        # if A is empty or X on A is same, then return a leaf node. The leaf node label is the most frequent label on X.
        elif len(A) == 0 or self.is_same_on_A(X, A):
            #print(index)
            return Node(isLeaf = True, attr=None, vals=[], label=np.argmax(np.bincount(y)))
        else:
            attr = self.biggest_info_gain(X, y, A)
            print(attr)
            #print(index)
            
            A_next = []
            for val in A:
                A_next.append(val)
            A_next.remove(attr)
            
            uniques = np.unique(self.X[:, attr], return_counts=False)
            node = Node(isLeaf=False, attr=attr, vals=uniques, label=None)
            for val in uniques:
                sub_X = X[X[:, attr] == val, :]
                if len(sub_X) == 0:
                    node.child.append(Node(isLeaf=True, attr=None, vals=[], label=np.argmax(np.bincount(y))))
                else:
                    node.child.append(self.generate_tree(X[X[:, attr] == val, :],
                                                     y[X[:, attr] == val], A_next, index[X[:, attr] == val]))
            return node
            
    # Print all the nodes of decision tree.
    def dfs(self, root):
        print(root.attr, root.isLeaf, root.label)
        for child in root.child:
            self.dfs(child)
    # Input a piece of data and return a predicted label for it.
    def predict(self, x):
        cur = self.root
        while not cur.isLeaf:
            cur = cur.child[np.argwhere(cur.vals == x[attr])]
        return cur.label
X = [[0, 0, 0, 0, 0, 0],
     [1, 0, 1, 0, 0, 0],
     [1, 0, 0, 0, 0, 0],
     [0, 0, 1, 0, 0, 0],
     [2, 0, 0, 0, 0, 0],
     [0, 1, 0, 0, 1, 1],
     [1, 1, 0, 1, 1, 1],
     [1, 1, 0, 0, 1, 0],
     [1, 1, 1, 1, 1, 0],
     [0, 2, 2, 0, 2, 1],
     [2, 2, 2, 2, 2, 0],
     [2, 0, 0, 2, 2, 1],
     [0, 1, 0, 1, 0, 0],
     [2, 1, 1, 1, 0, 0],
     [1, 1, 0, 0, 1, 1],
     [2, 0, 0, 2, 2, 0],
     [0, 0, 1, 1, 1, 0]]
y = [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]           
import numpy as np
X = np.array(X)
y = np.array(y)
# Create a new decision tree object.
dtree = Decision_Tree(X, y)
# Build a new tree decision tree.
dtree.build_tree()
# Check out whether your decision tree are built correctly or not.
dtree.dfs(dtree.root)
